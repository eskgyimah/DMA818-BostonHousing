{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMA818 Machine Learning Project\n",
    "## Principal Component Analysis and Principal Component Regression on Boston Housing Dataset\n",
    "\n",
    "**Student Name:** Edward Solomon Kweku Gyimah  \n",
    "**Student ID:** SE/DAT/24/0007  \n",
    "**Course:** DMA818 Machine Learning  \n",
    "**Institution:** University of Cape Coast - School of Economics  \n",
    "**Date:** September 2025  \n",
    "\n",
    "### Project Objective\n",
    "This project analyzes the Boston Housing dataset using Principal Component Analysis (PCA) to determine the optimal number of components for meaningful data representation, then builds a Principal Component Regression (PCR) model to predict median house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing dataset\n",
    "data = pd.read_csv('BostonHousing.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Boston Housing Dataset Overview\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nColumn names and types:\")\n",
    "print(data.dtypes)\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset description and summary statistics\n",
    "print(\"Dataset Description:\")\n",
    "print(\"=\" * 20)\n",
    "print(\"CRIM: Per capita crime rate by town\")\n",
    "print(\"ZN: Proportion of residential land zoned for lots over 25,000 sq.ft\")\n",
    "print(\"INDUS: Proportion of non-retail business acres per town\")\n",
    "print(\"CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\")\n",
    "print(\"NOX: Nitric oxides concentration (parts per 10 million)\")\n",
    "print(\"RM: Average number of rooms per dwelling\")\n",
    "print(\"AGE: Proportion of owner-occupied units built prior to 1940\")\n",
    "print(\"DIS: Weighted distances to employment centres\")\n",
    "print(\"RAD: Index of accessibility to radial highways\")\n",
    "print(\"TAX: Property tax rate per $10,000\")\n",
    "print(\"PTRATIO: Pupil-teacher ratio by town\")\n",
    "print(\"LSTAT: % lower status of the population\")\n",
    "print(\"MEDV: Median value of owner-occupied homes in $1000's (TARGET)\")\n",
    "print(\"CAT. MEDV: Categorical median value (0: â‰¤median, 1: >median)\")\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data quality\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(\"=\" * 25)\n",
    "print(\"Missing values per column:\")\n",
    "print(data.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {data.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {data.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = data.select_dtypes(include=[np.number]).corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': .8})\n",
    "plt.title('Boston Housing Dataset - Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features with MEDV\n",
    "medv_corr = correlation_matrix['MEDV'].abs().sort_values(ascending=False)\n",
    "print(\"Features most correlated with MEDV (target):\")\n",
    "print(medv_corr[1:])  # Exclude MEDV itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of target variable (MEDV)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(data['MEDV'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Median House Values (MEDV)', fontweight='bold')\n",
    "axes[0].set_xlabel('Median Value ($1000s)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(data['MEDV'].mean(), color='red', linestyle='--', label=f'Mean: ${data[\"MEDV\"].mean():.1f}k')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(data['MEDV'])\n",
    "axes[1].set_title('Box Plot of Median House Values', fontweight='bold')\n",
    "axes[1].set_ylabel('Median Value ($1000s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"MEDV Statistics:\")\n",
    "print(f\"Mean: ${data['MEDV'].mean():.2f}k\")\n",
    "print(f\"Median: ${data['MEDV'].median():.2f}k\")\n",
    "print(f\"Standard Deviation: ${data['MEDV'].std():.2f}k\")\n",
    "print(f\"Range: ${data['MEDV'].min():.1f}k - ${data['MEDV'].max():.1f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing for PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target variable\n",
    "# Exclude 'CAT. MEDV' as it's derived from MEDV\n",
    "feature_cols = [col for col in data.columns if col not in ['MEDV', 'CAT. MEDV']]\n",
    "X = data[feature_cols].copy()\n",
    "y = data['MEDV'].copy()\n",
    "\n",
    "print(f\"Features for analysis: {feature_cols}\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Display feature statistics before scaling\n",
    "print(\"\\nFeature statistics before scaling:\")\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features for PCA (important for PCA to work properly)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols)\n",
    "\n",
    "print(\"Features after standardization:\")\n",
    "print(f\"Mean values: {X_scaled_df.mean().round(10)}\")\n",
    "print(f\"Standard deviations: {X_scaled_df.std().round(10)}\")\n",
    "\n",
    "# Verify standardization\n",
    "print(\"\\nStandardization verification (should be ~0 and ~1):\")\n",
    "print(f\"Mean of all features: {np.mean(X_scaled):.10f}\")\n",
    "print(f\"Standard deviation of all features: {np.std(X_scaled, ddof=1):.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA with all components first\n",
    "pca_full = PCA()\n",
    "X_pca_full = pca_full.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate explained variance ratio\n",
    "explained_variance_ratio = pca_full.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "print(\"PCA Results - Explained Variance:\")\n",
    "print(\"=\" * 35)\n",
    "for i, (ev, cv) in enumerate(zip(explained_variance_ratio, cumulative_variance_ratio)):\n",
    "    print(f\"PC{i+1}: {ev:.4f} ({ev*100:.2f}%) - Cumulative: {cv:.4f} ({cv*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio,\n",
    "             'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_title('Scree Plot - Individual Explained Variance', fontweight='bold')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add elbow detection line\n",
    "axes[0].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='5% threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[1].plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio,\n",
    "             'ro-', linewidth=2, markersize=8)\n",
    "axes[1].set_title('Cumulative Explained Variance', fontweight='bold')\n",
    "axes[1].set_xlabel('Principal Component')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance Ratio')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add threshold lines for common variance retention levels\n",
    "for threshold, color in [(0.80, 'green'), (0.85, 'orange'), (0.90, 'purple'), (0.95, 'brown')]:\n",
    "    axes[1].axhline(y=threshold, color=color, linestyle='--', alpha=0.7,\n",
    "                   label=f'{threshold*100:.0f}% variance')\n",
    "\n",
    "axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of components using different criteria\n",
    "print(\"Optimal Number of Components Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Kaiser criterion (eigenvalue > 1)\n",
    "eigenvalues = pca_full.explained_variance_\n",
    "kaiser_components = np.sum(eigenvalues > 1)\n",
    "print(f\"1. Kaiser Criterion (eigenvalue > 1): {kaiser_components} components\")\n",
    "\n",
    "# 2. Variance thresholds\n",
    "for threshold in [0.80, 0.85, 0.90, 0.95]:\n",
    "    n_components = np.argmax(cumulative_variance_ratio >= threshold) + 1\n",
    "    print(f\"2. {threshold*100:.0f}% variance explained: {n_components} components\")\n",
    "\n",
    "# 3. Scree plot elbow (components with >5% individual variance)\n",
    "scree_components = np.sum(explained_variance_ratio > 0.05)\n",
    "print(f\"3. Scree plot criterion (>5% individual variance): {scree_components} components\")\n",
    "\n",
    "print(\"\\nDetailed eigenvalues:\")\n",
    "for i, ev in enumerate(eigenvalues):\n",
    "    print(f\"PC{i+1}: eigenvalue = {ev:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of component loadings for interpretation\n",
    "# Choose optimal number of components (we'll use 6 based on Kaiser criterion and 85% variance)\n",
    "n_components_optimal = 6\n",
    "pca_optimal = PCA(n_components=n_components_optimal)\n",
    "X_pca_optimal = pca_optimal.fit_transform(X_scaled)\n",
    "\n",
    "# Create loadings matrix\n",
    "loadings = pd.DataFrame(\n",
    "    pca_optimal.components_.T,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components_optimal)],\n",
    "    index=feature_cols\n",
    ")\n",
    "\n",
    "print(f\"Component Loadings Matrix ({n_components_optimal} components):\")\n",
    "print(\"=\" * 50)\n",
    "print(loadings.round(3))\n",
    "\n",
    "# Explained variance for optimal components\n",
    "print(f\"\\nExplained variance with {n_components_optimal} components:\")\n",
    "print(f\"Individual: {pca_optimal.explained_variance_ratio_}\")\n",
    "print(f\"Cumulative: {np.sum(pca_optimal.explained_variance_ratio_):.4f} ({np.sum(pca_optimal.explained_variance_ratio_)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize component loadings\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(n_components_optimal):\n",
    "    pc_loadings = loadings[f'PC{i+1}'].sort_values(key=abs, ascending=False)\n",
    "\n",
    "    colors = ['red' if x < 0 else 'blue' for x in pc_loadings.values]\n",
    "    bars = axes[i].barh(range(len(pc_loadings)), pc_loadings.values, color=colors, alpha=0.7)\n",
    "    axes[i].set_yticks(range(len(pc_loadings)))\n",
    "    axes[i].set_yticklabels(pc_loadings.index, fontsize=10)\n",
    "    axes[i].set_xlabel('Loading Value')\n",
    "    axes[i].set_title(f'PC{i+1} Loadings\\n(Explained Var: {pca_optimal.explained_variance_ratio_[i]:.3f})',\n",
    "                     fontweight='bold')\n",
    "    axes[i].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, pc_loadings.values):\n",
    "        if abs(value) > 0.1:  # Only label significant loadings\n",
    "            axes[i].text(value + (0.02 if value > 0 else -0.02), bar.get_y() + bar.get_height()/2,\n",
    "                        f'{value:.2f}', ha='left' if value > 0 else 'right', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Principal Component Regression (PCR) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"Training target mean: ${y_train.mean():.2f}k\")\n",
    "print(f\"Testing target mean: ${y_test.mean():.2f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train PCR model with optimal number of components\n",
    "pca_model = PCA(n_components=n_components_optimal)\n",
    "X_train_pca = pca_model.fit_transform(X_train)\n",
    "X_test_pca = pca_model.transform(X_test)\n",
    "\n",
    "# Fit linear regression on principal components\n",
    "pcr_model = LinearRegression()\n",
    "pcr_model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = pcr_model.predict(X_train_pca)\n",
    "y_test_pred = pcr_model.predict(X_test_pca)\n",
    "\n",
    "print(\"Principal Component Regression Model Trained Successfully\")\n",
    "print(f\"Using {n_components_optimal} principal components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation metrics\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\n{dataset_name} Set Performance:\")\n",
    "    print(f\"RÂ² Score: {r2:.4f}\")\n",
    "    print(f\"RMSE: ${rmse:.3f}k\")\n",
    "    print(f\"MAE: ${mae:.3f}k\")\n",
    "    print(f\"MSE: {mse:.3f}\")\n",
    "\n",
    "    return {'R2': r2, 'RMSE': rmse, 'MAE': mae, 'MSE': mse}\n",
    "\n",
    "print(\"PCR Model Performance Evaluation:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "train_metrics = calculate_metrics(y_train, y_train_pred, \"Training\")\n",
    "test_metrics = calculate_metrics(y_test, y_test_pred, \"Testing\")\n",
    "\n",
    "# Calculate generalization performance\n",
    "overfitting = train_metrics['R2'] - test_metrics['R2']\n",
    "print(f\"\\nGeneralization Analysis:\")\n",
    "print(f\"Overfitting (Train RÂ² - Test RÂ²): {overfitting:.4f}\")\n",
    "if overfitting < 0.05:\n",
    "    print(\"Model shows good generalization (minimal overfitting)\")\n",
    "elif overfitting < 0.10:\n",
    "    print(\"Model shows acceptable generalization\")\n",
    "else:\n",
    "    print(\"Model may be overfitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation for robust performance assessment\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create pipeline for cross-validation\n",
    "pcr_pipeline = Pipeline([\n",
    "    ('pca', PCA(n_components=n_components_optimal)),\n",
    "    ('regression', LinearRegression())\n",
    "])\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(pcr_pipeline, X_scaled, y, cv=5, scoring='r2')\n",
    "cv_scores_neg_mse = cross_val_score(pcr_pipeline, X_scaled, y, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_rmse = np.sqrt(-cv_scores_neg_mse)\n",
    "\n",
    "print(\"Cross-Validation Results (5-fold):\")\n",
    "print(\"=\" * 35)\n",
    "print(f\"RÂ² scores: {cv_scores}\")\n",
    "print(f\"Mean RÂ²: {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})\")\n",
    "print(f\"RMSE scores: {cv_rmse}\")\n",
    "print(f\"Mean RMSE: ${cv_rmse.mean():.3f}k (Â±${cv_rmse.std()*2:.3f}k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model predictions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Training predictions scatter plot\n",
    "axes[0,0].scatter(y_train, y_train_pred, alpha=0.6, color='blue')\n",
    "axes[0,0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0,0].set_xlabel('Actual MEDV ($1000s)')\n",
    "axes[0,0].set_ylabel('Predicted MEDV ($1000s)')\n",
    "axes[0,0].set_title(f'Training Predictions\\nRÂ² = {train_metrics[\"R2\"]:.3f}', fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Testing predictions scatter plot\n",
    "axes[0,1].scatter(y_test, y_test_pred, alpha=0.6, color='green')\n",
    "axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0,1].set_xlabel('Actual MEDV ($1000s)')\n",
    "axes[0,1].set_ylabel('Predicted MEDV ($1000s)')\n",
    "axes[0,1].set_title(f'Testing Predictions\\nRÂ² = {test_metrics[\"R2\"]:.3f}', fontweight='bold')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residual plots\n",
    "train_residuals = y_train - y_train_pred\n",
    "test_residuals = y_test - y_test_pred\n",
    "\n",
    "axes[1,0].scatter(y_train_pred, train_residuals, alpha=0.6, color='blue')\n",
    "axes[1,0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1,0].set_xlabel('Predicted MEDV ($1000s)')\n",
    "axes[1,0].set_ylabel('Residuals')\n",
    "axes[1,0].set_title('Training Residuals', fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1,1].scatter(y_test_pred, test_residuals, alpha=0.6, color='green')\n",
    "axes[1,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1,1].set_xlabel('Predicted MEDV ($1000s)')\n",
    "axes[1,1].set_ylabel('Residuals')\n",
    "axes[1,1].set_title('Testing Residuals', fontweight='bold')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Interpretation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze principal component contributions to the model\n",
    "pc_coefficients = pcr_model.coef_\n",
    "pc_names = [f'PC{i+1}' for i in range(n_components_optimal)]\n",
    "\n",
    "print(\"Principal Component Coefficients in PCR Model:\")\n",
    "print(\"=\" * 45)\n",
    "coef_df = pd.DataFrame({\n",
    "    'Component': pc_names,\n",
    "    'Coefficient': pc_coefficients,\n",
    "    'Abs_Coefficient': np.abs(pc_coefficients)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "# Visualize PC coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red' if x < 0 else 'blue' for x in coef_df['Coefficient']]\n",
    "bars = plt.bar(coef_df['Component'], coef_df['Coefficient'], color=colors, alpha=0.7)\n",
    "plt.title('Principal Component Coefficients in PCR Model', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Principal Components')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, coef_df['Coefficient']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, value + (0.1 if value > 0 else -0.1),\n",
    "             f'{value:.2f}', ha='center', va='bottom' if value > 0 else 'top', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret components in terms of original features\n",
    "print(\"Principal Component Interpretation:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Get the most influential features for each PC\n",
    "for i in range(n_components_optimal):\n",
    "    pc_loadings = loadings[f'PC{i+1}'].abs().sort_values(ascending=False)\n",
    "    coef = pc_coefficients[i]\n",
    "\n",
    "    print(f\"\\nPC{i+1} (Coefficient: {coef:.3f}, Variance: {pca_optimal.explained_variance_ratio_[i]:.3f}):\")\n",
    "    print(f\"Top contributing features:\")\n",
    "\n",
    "    for j, (feature, loading) in enumerate(pc_loadings.head(3).items()):\n",
    "        original_loading = loadings.loc[feature, f'PC{i+1}']\n",
    "        print(f\"  {j+1}. {feature}: {original_loading:.3f} (|{loading:.3f}|)\")\n",
    "\n",
    "    # Interpretation based on top features and coefficient sign\n",
    "    top_features = pc_loadings.head(3).index.tolist()\n",
    "    if i == 0:  # Usually captures the most variance\n",
    "        print(f\"  Interpretation: Likely represents overall housing quality/socioeconomic status\")\n",
    "    elif i == 1:\n",
    "        print(f\"  Interpretation: Possibly related to location/accessibility factors\")\n",
    "    else:\n",
    "        print(f\"  Interpretation: Captures specific housing/neighborhood characteristics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare PCR with standard multiple linear regression\n",
    "print(\"Comparison with Standard Multiple Linear Regression:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fit standard MLR\n",
    "mlr_model = LinearRegression()\n",
    "mlr_model.fit(X_train, y_train)\n",
    "y_train_mlr = mlr_model.predict(X_train)\n",
    "y_test_mlr = mlr_model.predict(X_test)\n",
    "\n",
    "# Calculate MLR metrics\n",
    "mlr_train_r2 = r2_score(y_train, y_train_mlr)\n",
    "mlr_test_r2 = r2_score(y_test, y_test_mlr)\n",
    "mlr_train_rmse = np.sqrt(mean_squared_error(y_train, y_train_mlr))\n",
    "mlr_test_rmse = np.sqrt(mean_squared_error(y_test, y_test_mlr))\n",
    "\n",
    "# Comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Train RÂ²', 'Test RÂ²', 'Train RMSE', 'Test RMSE', 'Overfitting'],\n",
    "    'PCR': [train_metrics['R2'], test_metrics['R2'], train_metrics['RMSE'],\n",
    "           test_metrics['RMSE'], train_metrics['R2'] - test_metrics['R2']],\n",
    "    'MLR': [mlr_train_r2, mlr_test_r2, mlr_train_rmse, mlr_test_rmse, mlr_train_r2 - mlr_test_r2]\n",
    "})\n",
    "\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "print(f\"\\nModel Complexity:\")\n",
    "print(f\"PCR uses {n_components_optimal} components vs MLR uses {len(feature_cols)} features\")\n",
    "print(f\"Dimensionality reduction: {((len(feature_cols) - n_components_optimal) / len(feature_cols)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"PRINCIPAL COMPONENT ANALYSIS AND REGRESSION SUMMARY\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(f\"\\n1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   â€¢ {data.shape[0]} housing records with {len(feature_cols)} predictive features\")\n",
    "print(f\"   â€¢ Target variable (MEDV) ranges from ${y.min():.1f}k to ${y.max():.1f}k\")\n",
    "print(f\"   â€¢ No missing values, clean dataset suitable for analysis\")\n",
    "\n",
    "print(f\"\\n2. PCA RESULTS:\")\n",
    "print(f\"   â€¢ Optimal number of components: {n_components_optimal} (based on Kaiser criterion)\")\n",
    "print(f\"   â€¢ Variance explained: {np.sum(pca_optimal.explained_variance_ratio_)*100:.1f}%\")\n",
    "print(f\"   â€¢ Dimensionality reduction: {((len(feature_cols) - n_components_optimal) / len(feature_cols)) * 100:.1f}% fewer dimensions\")\n",
    "print(f\"   â€¢ Most important PC: PC1 explains {pca_optimal.explained_variance_ratio_[0]*100:.1f}% of variance\")\n",
    "\n",
    "print(f\"\\n3. PCR MODEL PERFORMANCE:\")\n",
    "print(f\"   â€¢ Test RÂ² Score: {test_metrics['R2']:.3f} (explains {test_metrics['R2']*100:.1f}% of variance)\")\n",
    "print(f\"   â€¢ Test RMSE: ${test_metrics['RMSE']:.2f}k (prediction error)\")\n",
    "print(f\"   â€¢ Cross-validation RÂ²: {cv_scores.mean():.3f} Â± {cv_scores.std()*2:.3f}\")\n",
    "print(f\"   â€¢ Generalization: {'Good' if overfitting < 0.05 else 'Acceptable' if overfitting < 0.10 else 'Poor'} (overfitting = {overfitting:.3f})\")\n",
    "\n",
    "print(f\"\\n4. MODEL INTERPRETATION:\")\n",
    "print(f\"   â€¢ PC1 likely represents overall housing quality/socioeconomic factors\")\n",
    "print(f\"   â€¢ PC2 appears related to location and accessibility characteristics\")\n",
    "print(f\"   â€¢ Model successfully captures main patterns in housing price variation\")\n",
    "print(f\"   â€¢ Reduced model complexity while maintaining predictive performance\")\n",
    "\n",
    "print(f\"\\n5. PRACTICAL IMPLICATIONS:\")\n",
    "print(f\"   â€¢ PCR provides interpretable, lower-dimensional representation\")\n",
    "print(f\"   â€¢ Useful for understanding key drivers of housing prices\")\n",
    "print(f\"   â€¢ Can help in feature selection and model simplification\")\n",
    "print(f\"   â€¢ Suitable for prediction with reduced computational complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic Reflection\n",
    "\n",
    "This analysis demonstrates the application of Principal Component Analysis (PCA) and Principal Component Regression (PCR) to real-world housing data. Key learning outcomes include:\n",
    "\n",
    "### Statistical Methodology:\n",
    "- **PCA Theory**: Understanding how PCA transforms correlated variables into uncorrelated components\n",
    "- **Dimensionality Reduction**: Practical application of variance-based component selection\n",
    "- **Regression on Components**: Using principal components as predictors instead of original features\n",
    "\n",
    "**Submitted by:** Edward Solomon Kweku Gyimah (SE/DAT/24/0007)  \n",
    "**Course:** DMA818 Machine Learning  \n",
    "**University of Cape Coast - School of Economics**  \n",
    "**September 2025**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
